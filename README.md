# Multicripto_Omega
El final de una era, el inicio de 6 profesionales

# Estructura del Repositorio
Este repositorio es el template de trabajo para el projecto final de Ciencia de Datos del equipo Multicripto. Esta dividido en 4 carpetas las cuales sirven para lo siguiente:
  ```
    ğŸ“¦multicripto_omega
     â”£ ğŸ“‚data
     â”ƒ â”£ ğŸ“‚Base_Data
     â”ƒ â”£ ğŸ“‚Clustered_Data
     â”ƒ â”£ ğŸ“‚Historic_Data
     â”£ ğŸ“‚models
     â”£ ğŸ“‚pipe
     â”ƒ â”£ ğŸ“‚test_dagster
     â”ƒ â”£ ğŸ“‚utils
     â”ƒ â”£ ğŸ“œ0_run_parameters.py
     â”ƒ â”£ ğŸ“œ1_etl.py
     â”ƒ â”£ ğŸ“œ2_clustering.py
     â”ƒ â”£ ğŸ“œ3_saving.py
     â”£ ğŸ“‚test
  ```

- [Data](data): Aqui se guardan los datos historicos y respaldos cuando se ejecuta la pipeline.

- [Models](models): Se guardan historicamente los .pkl con los mejores modelos de clustering entrenados en cada ejecucion.

- [Pipe](pipe): Esta es la carpeta importante y a revisar. Esta el cÃ³digo de la pipeline pero sera explicado mÃ¡s a detalle abajo.

- [Test](test): Es una carpeta en donde tenemos algunos intentos de pruebas varias que hicimos. No hay ningun cÃ³digo oficial aqui.


# Pipeline

Esta es la carpeta principal y esta dividida en 2 partes. Primero tenemos el cÃ³digo que se ejecutaria si quisieras correr la pipeline en una terminal (Ir script por script ejecutando manualmente) y creemos que puede ser la forma mÃ¡s amigable de revisar el cÃ³digo. Luego tenemos otra carpeta [dagster_test](pipe/dagster_test) la cual tiene lo mismo pero escrito para ser montado con dagster para orquestrar el flujo. 

## CreaciÃ³n de la pipeline
Tenemos 4 scripts importantes y una carpeta [utils](pipe/utils) que hacen lo siguiente:

- [Carpeta utils](pipe/utils): Guardamos codigo auxiliar a la pipeline y tiene la estructura:
    ```
   ğŸ“‚utils
    â”£ ğŸ“œdatawrangling_utils.py
    â”£ ğŸ“œQuery-Reservaciones.sql
    â”£ ğŸ“œrun_parameters.json
    ```
    Aqui guardamos la query de SQL que se ejecuta para traer la informaciÃ³n, algunas funciones auxiliares de data wrangling y aqui tambien se guarda el .json con los parametros de la ejecucion actual.

- [0_run_parameters](pipe/0_run_parameters.py): Definimos metaparamentros de la ejecucion como el tiempo de inicio y un uuid.
- [1_etl.py](pipe/1_etl.py): Nos conectamos a la base de datos, hacemos la query y limpiamos los datos. Guardamos esto por persistencia en la carpeta de `data/Base_Data`.
- [2_clustering.py](pipe/2_clustering.py): Se prueban varios algoritmos distintos de clustering y se define el mejor. Luego guardamos el modelo resultante en `models` y los resultados del clustering en `data/Clustered_Data`.
- [3_saving.py](pipe/3_saving.py): Agregamos a la informaciÃ³n la data de ocupaciÃ³n. Nos conectamos de nuevo a la base de datos y actualizamos la tabla de resultados con el dataframe final. Tambien guardamos un backup en `data/Historic_Data`.
  

## Implementacion con Dagster
Aqui tenemos una carpeta llamada `pipeline_rh` la cual contiene la pipeline. Tambien aqui tenemos la informacion de un venv que para ejecutar la pipeline pero esto se omitiÃ³ del repositorio. (los requirements estan en la carpeta principal). 
La estructura es la siguiente:

  ```
    ğŸ“¦pipe
     â”£ ğŸ“‚dagster_test
     â”ƒ â”£ ğŸ“‚pipeline_rh
     â”ƒ â”ƒ â”£ ğŸ“‚data
     â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚Base_Data
     â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ.gitkeep
     â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œTransformed_df_2024-06-03_H22-M03-S52_ccd8ec6a-3cc1-48e6-b05b-2f5783fa601f.parquet
     â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œTransformed_df_2024-06-06_H12-M20-S16_e297b21a-9803-4d71-b0bb-76fe9e3d6865.parquet
     â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚Clustered_Data
     â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ.gitkeep
     â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œClustered_df_2024-06-03_H22-M03-S52_ccd8ec6a-3cc1-48e6-b05b-2f5783fa601f.parquet
     â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œClustered_df_2024-06-06_H12-M20-S16_e297b21a-9803-4d71-b0bb-76fe9e3d6865.parquet
     â”ƒ â”ƒ â”ƒ â”£ ğŸ“‚Historic_Data
     â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ.gitkeep
     â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œHistoric_df_2024-06-03_H22-M03-S52_ccd8ec6a-3cc1-48e6-b05b-2f5783fa601f.parquet
     â”ƒ â”ƒ â”ƒ â”ƒ â”£ ğŸ“œHistoric_df_2024-06-06_H12-M20-S16_e297b21a-9803-4d71-b0bb-76fe9e3d6865.parquet
     â”ƒ â”ƒ â”£ ğŸ“‚models
     â”ƒ â”ƒ â”ƒ â”£ ğŸ“œ.gitkeep
     â”ƒ â”ƒ â”ƒ â”£ ğŸ“œfinal_model_DBSCAN_2024-06-03_H22-M03-S52_ccd8ec6a-3cc1-48e6-b05b-2f5783fa601f.pkl
     â”ƒ â”ƒ â”ƒ â”— ğŸ“œfinal_model_Mixture_2024-06-06_H12-M20-S16_e297b21a-9803-4d71-b0bb-76fe9e3d6865.pkl
     â”ƒ â”ƒ â”£ ğŸ“‚utils
     â”ƒ â”ƒ â”£ ğŸ“‚pipeline_rh
     â”ƒ â”ƒ â”ƒ â”£ ğŸ“œassets.py
     â”ƒ â”ƒ â”ƒ â”— ğŸ“œ__init__.py
     â”ƒ â”ƒ â”£ ğŸ“‚pipeline_rh.egg-info
     â”ƒ â”ƒ â”£ ğŸ“œpyproject.toml
     â”ƒ â”ƒ â”£ ğŸ“œREADME.md
     â”ƒ â”ƒ â”£ ğŸ“œsetup.cfg
     â”ƒ â”ƒ â”— ğŸ“œsetup.py
  ```

Tenemos las mismas carpetas `data`, `models` y `utils` que son analogas a las otras en el repositorio. Para guardar datos y definir funciones. 

luego esta la carpetas
- [pipeline_rh](pipe/dagster_test/pipeline_rh/pipeline_rh) Tenemos 2 cÃ³digos.
    - [__init__.py](pipe/dagster_test/pipeline_rh/pipeline_rh/__init__.py): Inicializador de todos los assets y schedules que se ejecutaran en dagster.
    - [assets.py](pipe/dagster_test/pipeline_rh/pipeline_rh/assets.py): Definimos los assets que son objetos de dagster para modificar una pipeline. En este caso nuestros assets son los scripts.

El resto de carpetas y archivos son de setup y configuracion para dagster y fueron generados por la libreria. El unico cambio que se hizÃ³ fue en [setup.py](pipe/dagster_test/pipeline_rh/setup.py) en donde tenemos que agregar todas las librerias en nuestro requirements.txt. 





